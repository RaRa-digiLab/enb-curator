{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "API_KEY = \"yourkeyhere\"\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(publisher_strings: list, ndim=1000, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "    input=publisher_strings,\n",
    "    model=model,\n",
    "    dimensions=ndim\n",
    ")\n",
    "    embeddings_list = [x.embedding for x in response.data]\n",
    "    embeddings = dict(zip(publisher_strings, embeddings_list))\n",
    "    return embeddings\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "def vector_based_clustering(embedding_dict, cosine_similarity_threshold=0.85, tqdm_message=\"\"):\n",
    "    \"\"\"Clusters publishers based on cosine similarity of their vectors and assigns a representative group name.\"\"\"\n",
    "    publishers = embedding_dict.keys()\n",
    "    \n",
    "    # Step 1: Initial clustering using vector cosine similarity\n",
    "    groups = []\n",
    "    for i, publisher in enumerate(tqdm(publishers, desc=tqdm_message)):\n",
    "        added_to_group = False\n",
    "        \n",
    "        for group in groups:\n",
    "            # Calculate cosine similarity with all members of the group\n",
    "            similarities = [cosine_similarity(embedding_dict[publisher], embedding_dict[member]) for member in group]\n",
    "            average_similarity = np.mean(similarities)\n",
    "\n",
    "            if average_similarity >= cosine_similarity_threshold:\n",
    "                group.append(publisher)\n",
    "                added_to_group = True\n",
    "                break\n",
    "        \n",
    "        if not added_to_group:\n",
    "            groups.append([publisher])\n",
    "    \n",
    "    # Step 2: Assign representative to each group and map to DataFrame\n",
    "    # Representative is chosen as the shortest name in the group\n",
    "    representative_mapping = {}\n",
    "    for group in groups:\n",
    "        representative = min(group, key=len)\n",
    "        for member in group:\n",
    "            representative_mapping[member] = representative\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_publisher_similarity_mapping(df, locations, cosine_similarity_threshold, output_path):\n",
    "\n",
    "    max_whitespace = len(sorted(locations, key=len, reverse=True)[0])\n",
    "\n",
    "    with open(output_path, \"a\", encoding=\"utf8\") as f:\n",
    "        f.write(\"publication_place_harmonized\\tpublisher_harmonized\\tpublisher_similarity_group\\n\")\n",
    "        \n",
    "        for i, location in enumerate(locations):\n",
    "            \n",
    "            try:\n",
    "                location_df = df.groupby(\"publication_place_harmonized\").get_group(location)\n",
    "                publishers = location_df[\"publisher_harmonized\"].unique()\n",
    "\n",
    "                embeddings = {}\n",
    "                batch_size = 2048\n",
    "\n",
    "                # Split publishers into batches if necessary\n",
    "                if len(publishers) <= batch_size:\n",
    "                    embeddings = get_embeddings(publishers, ndim=1000, model=\"text-embedding-3-small\")\n",
    "                else:\n",
    "                    for j in range(0, len(publishers), batch_size):\n",
    "                        batch = publishers[j:j + batch_size]\n",
    "                        batch_embeddings = get_embeddings(batch, ndim=1000, model=\"text-embedding-3-small\")\n",
    "                        \n",
    "                        # Extend the embeddings dictionary with new batch\n",
    "                        embeddings.update(batch_embeddings)\n",
    "\n",
    "                groups = vector_based_clustering(embeddings,\n",
    "                                                 cosine_similarity_threshold,\n",
    "                                                 tqdm_message=f\"{i}/{len(locations)} - {location} {' '*(max_whitespace - len(location))}\")\n",
    "                \n",
    "                for group in sorted(groups, key=len, reverse=True):\n",
    "                    representative = min(group)\n",
    "                    for publisher in group:\n",
    "                        f.write(f\"{location}\\t{publisher}\\t{representative}\\n\")\n",
    "                \n",
    "                # Flush data to disk after each location\n",
    "                f.flush()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed processing {location}: {e}\")\n",
    "\n",
    "    print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector-based clustering procedure starts from the current state of the curated dataset, using harmonized places and publishers\n",
    "\n",
    "enb = pd.read_parquet(\"../data/curated/enb_books.parquet\")\n",
    "df = enb[[\"publication_place_harmonized\", \"publisher_harmonized\"]].drop_duplicates().dropna()\n",
    "locations = df[\"publication_place_harmonized\"].value_counts()[lambda x: x > 1].index[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to start requesting the OpenAI API for embeddings and perform the clustering\n",
    "# The results are stored in the relevant config file\n",
    "create_publisher_similarity_mapping(df, locations, 0.7, \"publisher_similarity_groups.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".curator_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
