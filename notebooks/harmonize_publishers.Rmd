---
title: "Harmonizing publisher names"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
    self_contained: no
editor_options:
  chunk_output_type: console
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../reports"
    )
  })
---

# Harmonizing publisher names

Last data file 08-10-2024, last updated processing 16-01-2025.

```{r setup,echo=F,include =F}
#knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```


```{r,include = F}
# Read the packages
suppressMessages(library(data.table))
suppressMessages(library(tidyverse))
suppressMessages(library(nanoparquet))
suppressMessages(library(stringdist))


# Read the parquet file and convert it into data.table. Remove the parquet instance.
works_parq <- nanoparquet::read_parquet("../data/curated/enb_all_books.parquet")
works <-  setDT(works_parq)
rm(works_parq)

# Read the places harmonized input file.
places_harmonized <- fread("../config/places/places_harmonized.tsv",sep="\t")

# Connect the data to add harmonized places.
works4 <- merge(works,places_harmonized,by.x="publication_place",by.y="place_original",all.x=T)

# Store the original name of the publisher.
works4[,publisher_orig:=publisher]

# We can remove one addition from the end already.
works4[,publisher_orig:=str_replace(publisher_orig, ", levitaja", "")]

# Sometimes there are several publishers separated by a semicolon, we make a table with one publisher name per row.
unlist_publishers <- works4[,.(publisher_unlist=unlist(str_split(publisher_orig,";"))),id][,publisher_unlist:=trimws(publisher_unlist)]

#Almost 6000 books have 2 or more publishers
unlist_publishers[,.(.N,publisher_unlist),id][N>1][,uniqueN(id)]

# We add extra information to the list.
works5 <- merge(works4,unlist_publishers, by="id")
publishers <- unique(works5[,.N,.(publisher_unlist,publication_date_cleaned,place_harmonized)][,n_works:=sum(N,na.rm=T),publisher_unlist][,years_publisher:=.N,publisher_unlist][years_publisher>10,big_pub:=T][,min_time:=min(publication_date_cleaned,na.rm=T),publisher_unlist][,max_time:=max(publication_date_cleaned,na.rm=T),publisher_unlist][order(publisher_unlist,-years_publisher)][publisher_unlist!=""][,-c("publication_date_cleaned","N")])

# And start with rules to clean up the publisher names. We create a column standardizing_name to work with and update.
publishers[,standardizing_name:=publisher_unlist]
publishers[,standardizing_name:=str_replace_all(standardizing_name,'""+','"')]
publishers[,standardizing_name:=str_replace_all(standardizing_name,",","")]
publishers[,standardizing_name:=str_replace_all(standardizing_name,"-","")]
publishers[,standardizing_name:=str_remove(standardizing_name,'^"+')]
publishers[,standardizing_name:=str_remove(standardizing_name,'"+$')]
publishers[,standardizing_name:=trimws(standardizing_name)]
publishers[,standardizing_name:=tolower(standardizing_name)]
publishers[,standardizing_name:=str_replace_all(standardizing_name,"\\. ",".")]
publishers[,standardizing_name:=str_replace_all(standardizing_name,"'i rmtkpl. ","")]
publishers[str_detect(standardizing_name,"\\["),standardizing_name:=str_replace_all(standardizing_name,"\\[","")]
publishers[str_detect(standardizing_name,"\\]"),standardizing_name:=str_replace_all(standardizing_name,"\\]","")]
publishers[,standardizing_name:=str_replace(standardizing_name,":$","")]
publishers[,standardizing_name:=str_replace(standardizing_name,";$","")]
publishers[,standardizing_name:=trimws(standardizing_name)]

# We read the rules file. Due to difficulties in storing regular expressions in external files, note that strip.white=F is important here. And after the backslashes that are added automatically in reading the file are converted to a smaller amount of them.
rules <- fread("../config/publishers/publisher_harmonize_rules.tsv",sep="\t",strip.white=F,quote="")
rules[,find_this:=str_replace_all(find_this,"\\\\{2}","\\\\")]

# We first replace the exact matches
merged <- merge(publishers,rules[type=="exact"],by.x="standardizing_name",by.y="find_this",all.x=T)
merged[type=="exact",standardizing_name:=replace_with]
#merged[type=="exact"]

# Then we take the rules where regular expressions are used to find matches and replace the whole string.
regexes <- rules[type=="regex_replace"]
for (i in 1:nrow(regexes)){
  merged[str_detect(standardizing_name,regexes[i,find_this]),replace_with:=regexes[i,replace_with]]
  merged[str_detect(standardizing_name,regexes[i,find_this]),type:="regex_replace"]
  merged[!is.na(replace_with),standardizing_name:=replace_with]
  merged[,standardizing_name:=trimws(standardizing_name)]
}

# Finally, we take regular expressions that are used to replace only the substring. The sequence by which this operation is done can matter for the final result, since the string can be updated several times.
regexes_partial <- rules[type=="regex_partial"]
for (i in 1:nrow(regexes_partial)){
  merged[str_detect(standardizing_name,regexes_partial[i,find_this]),replace_with:=str_replace(standardizing_name,regexes_partial[i,find_this],regexes_partial[i,replace_with])]
  merged[str_detect(standardizing_name,regexes_partial[i,find_this]),type:="regex_partial"]
  merged[!is.na(replace_with),standardizing_name:=replace_with]
  merged[,standardizing_name:=trimws(standardizing_name)]
}

# Finally, extra punctation is removed from beginning and end of the string.
merged[str_detect(standardizing_name,"\\'$"),standardizing_name:=str_replace(standardizing_name,"\\'$","")]
merged[str_detect(standardizing_name,"\\.$"),standardizing_name:=str_replace(standardizing_name,"\\.$","")]
merged[str_detect(standardizing_name,"^\\("),standardizing_name:=str_replace(standardizing_name,"^\\(","")]
merged[str_detect(standardizing_name,"\\)$"),standardizing_name:=str_replace(standardizing_name,"\\)$","")]

# For missing publisher names, a unified name 's.n' is given.
merged[standardizing_name=="",standardizing_name:="s.n"]

# For an overview of the dataset, the standardized publisher names are described by simple statistics.
merged[,n_types:=.N,standardizing_name]#[N>1][,sum(N)]
merged[,n_sum:=sum(n_works),standardizing_name]
merged[n_sum>100,big_pub2:=n_sum>100,standardizing_name]

# Cyrillic names have not beet standardized in the rulebased approach. These are main statistics for non-cyrillic names.
merged[!str_detect(standardizing_name,"\\p{Cyrillic}"),uniqueN(publisher_unlist)]
merged[!str_detect(standardizing_name,"\\p{Cyrillic}"),uniqueN(standardizing_name)]
merged[!str_detect(standardizing_name,"\\p{Cyrillic}"),.N,standardizing_name][N>1][,sum(N)]

# There are a number of big publishers here with more than 40 works for the variant name.  An alternative measure is also proposed above for big_pub2 with more than 100 works across the standardized name.
merged[big_pub==T]
merged[,count_standardized:=.N,.(standardizing_name,place_harmonized)]

# For simplicity we reduce very long publisher names to their first 50 characters.
merged[,standardizing_name:=substr(standardizing_name,0,50)]
merged[,publisher_unlist:=substr(publisher_unlist,0,50)]

# An external table was built on the basis of this that was used for the text embedding approach to look for further similarities between publisher names.
fwrite(unique(merged[,.(publisher_original=publisher_unlist,publisher_harmonized=standardizing_name)]),"../config/publishers/publisher_harmonization_mapping.tsv",sep="\t")
fwrite(merged[order(place_harmonized,publisher_harmonized=standardizing_name)],"../reports/publishers_overview_rulebased_harmonization.tsv",sep="\t")


# The results of the text embedding approach are read in here.
publisher_clusters <- fread("../config/publishers/publisher_similarity_groups.tsv",sep="\t",strip.white = F)
bothmethods <- unique(merge(unique(merged[,.(standardizing_name,publisher_unlist)]),publisher_clusters[,.(standardizing_name,similarity_group)],by.x=c("standardizing_name"),by.y=c("standardizing_name"),all=T))
see <- bothmethods[is.na(publisher_unlist)]

# Some basic statistics on both cases
bothmethods[,.(uniqueN(publisher_unlist))]
bothmethods[,.(uniqueN(standardizing_name))]
(bothmethods[,.(uniqueN(publisher_unlist))]-bothmethods[,.(uniqueN(standardizing_name))])/bothmethods[,.(uniqueN(publisher_unlist))]
bothmethods[is.na(similarity_group),similarity_group:=standardizing_name]
bothmethods[,.(uniqueN(similarity_group))]
merged[,uniqueN(standardizing_name)]
bothmethods[,.(uniqueN(similarity_group))]/bothmethods[,.(uniqueN(publisher_unlist))]
1-(bothmethods[,.(uniqueN(similarity_group))]/bothmethods[,.(uniqueN(publisher_unlist))])-((bothmethods[,.(uniqueN(publisher_unlist))]-bothmethods[,.(uniqueN(standardizing_name))])/bothmethods[,.(uniqueN(publisher_unlist))])

# We created a separate testset for the harmonizations in one Estonian city, Viljandi.
#cluster_similarity <- fread("../reports/viljandi_clusters_similarity07.tsv",sep="\t")
#cluster_similarity[,N:=.N,groups]
#fwrite(cluster_similarity[N>1],"data/cluster_similarity_check.tsv",sep="\t")

# They were manually checked for the potential usefulness of the link made. The results are stored in this file.
cluster_similarity_checked <- fread("../reports/testsets/testset_publishers_cluster_similarity_checked.tsv",sep="\t")
cluster_similarity_checked[standardizing_name==groups,useful_link:=NA]
cluster_similarity_checked[,.N,useful_link]

sum(cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][,N])
cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][useful_link=="T",N]
cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][useful_link=="F",N]

# Similar check was made for the pure rulebased approach for the publishers from Viljandi
all_publishers <- fread("../reports/publishers_overview_rulebased_harmonization.tsv",sep="\t")
only_rulebased <- merge(all_publishers[place_harmonized=="Viljandi"],cluster_similarity_checked,by="standardizing_name")[,.(publisher_unlist,standardizing_name)]
only_rulebased[,N:=.N,standardizing_name]
check_rulebased <- unique(only_rulebased[N>1])
#fwrite(check_rulebased,"data/rulebased_check.tsv",sep="\t")

# The results are stored in a separate file
rulebased_checked <- fread("../reports/testsets/testset_publishers_rulebased_checked.tsv",sep="\t")
rulebased_checked[, selflink := rowid(standardizing_name)==1]
rulebased_checked[selflink!=T,.N,useful_link][,N]

# We created a summary table of both results here.
both_methods <- merge(all_publishers[place_harmonized=="Viljandi"],cluster_similarity_checked,by="standardizing_name")[,.(publisher_unlist,standardizing_name,groups,useful_link)]
both_methods2 <- merge(both_methods,rulebased_checked,by=c("standardizing_name","publisher_unlist"),all=T)
#fwrite(both_methods2[order(groups)],"../reports/testsets/testset_publishers_harmonize_both_methods_summary.tsv",sep="\t")

# While in the original dataset the number of publisher name variants in the city of Viljandi was 516.
all_publishers[place_harmonized=="Viljandi"][,uniqueN(publisher_unlist)]


```

One of the most challenging aspects of data harmonization lies in publishers’ information. Although it holds significant research potential, publisher data is often noisy, with a high degree of variability. Publishers can appear as individuals, organizations, or publishing houses, each with multiple name variations. To harmonize this information, we employed two methods – rule-based and vector similarity-based approaches – and kept the results as separate columns. 

In the rule-based approach, we crafted over 300 regular expressions to standardize publisher names. These patterns handled a range of tasks, such as removing common suffixes (e.g., ‘printing,’ ‘& co.’) and standardizing variants of prominent publishers. This application of rules successfully reduced variance in the publishers column by `r scales::percent((merged[,uniqueN(publisher_unlist)]-merged[,uniqueN(standardizing_name)])/merged[,uniqueN(publisher_unlist)],0.1)`, from `r scales::comma(merged[,uniqueN(publisher_unlist)])` unique entries to `r scales::comma(merged[,uniqueN(standardizing_name)])`. 

For the second approach, we leveraged the rule-harmonized publisher names to create semantic embeddings. Using OpenAI’s text-embedding-3-small model (OpenAI 2023), we generated 1000-dimensional vectors for each name form. We then clustered publishers within each location based on cosine similarity, using harmonized place names as an anchor. Operating under the assumption that many publishers, particularly those contributing to data variability, are active in a single location, we set a relatively low cosine similarity threshold (0.7) to encourage more inclusive clustering. The vector-based grouping brought together similar publisher names in each location, regardless of length or language. This approach further reduced the variance in publishers by an additional `r scales::percent(unlist((merged[,uniqueN(publisher_unlist)]-bothmethods[,.(uniqueN(similarity_group))])/merged[,uniqueN(publisher_unlist)]-(merged[,uniqueN(publisher_unlist)]-merged[,uniqueN(standardizing_name)])/merged[,uniqueN(publisher_unlist)]),0.1)`, leaving `r scales::comma(unlist(bothmethods[,.(uniqueN(similarity_group))]))` unique names or 682% of the original number of unique entries.


To evaluate the workflow’s quality, we created a test set focused on one city in Estonia, Viljandi. The rule-based approach proved conservative but accurate, yielding `r rulebased_checked[selflink!=T,.N,useful_link][,N]` correct links and no errors across `r all_publishers[place_harmonized=="Viljandi"][,uniqueN(publisher_unlist)]` publisher names. The vector-based method contributed an additional `r sum(cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][,N])` links: `r cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][useful_link=="T",N]` were clearly correct, while `r cluster_similarity_checked[,.N,useful_link][!is.na(useful_link)][useful_link=="F",N]` were ambiguous or incorrect. The ambiguous links typically connected publishers with related meanings but distinct identities (e.g., societies from different professions within the same city). Table 1 shows examples of the links made in either approach. Whether these links are useful will depend on the analyst’s goals; we recommend that users manually review these connections to assess their relevance. 

