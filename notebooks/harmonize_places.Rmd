---
title: "Harmonizing place names"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
    self_contained: no
editor_options:
  chunk_output_type: console
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../reports"
    )
  })
---

# Harmonizing place names

Last data file 08-10-2024, last updated processing 16-01-2025.

```{r setup,echo=F}
#knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

Catalogue data entry practices typically aim for precision. This means that publisher names and locations are entered as originally printed, where substantial variation in presentation can be present. In a long-spanning dataset this can include historical variants of placenames (e.g. Dorpat, Tarbatu, Derpt, Jurjev, Tartu), grammar (Tartus, Tartun, In Tartu), localized versions of placenames (Tartto, Tērbata, Тарту), historical spelling variants (Tarto-linan, Tartolinan, Iourieff, Jujew). The entries often have additional specifications too, including other variants (Tartu – Dorpat, Youriev (Tartu)), wider geographical areas (Tartu (Tartumaa)), markers for other places of publication (etc, jne), publisher name placeholders (Tartu:[s.n]) and even several placenames sometimes (Tartu [i.e. Torino], Tallinn [p.o. Tartu]). These variants present a challenge for data analysis, as the data field needs to be harmonized in aggregating all books that were, for example, published in Tartu. 

Data harmonization is a common task for adapting catalogue data for computational analysis, where a general framework suitable for any dataset is difficult to offer. The historical variants and spelling variants cannot effectively be derived from the placenames, with even predictable variants relying on language-specific patterns. Additionally, the extent of variation within a particular dataset can be difficult to predict and will inevitably rely on custom effort in each case. There are some general patterns: for example, sometimes the placenames include "Printed in", or "etc", sometimes they rely on local grammar, books printed in another language are likely to include a localized version of the placename, however even their adaptation is language and case dependent. 

To harmonize the placenames, we built a general workflow that relies on external geographical databases that contain the relevant placenames as well as their historical variants. To do this, we first removed the grammatical markers from the placenames where possible. In Estonian case, the frequent forms here were placenames ending with an s, which were extracted from the set and manually annotated for nominative variants of the names. We then used several geocoding services to link the placenames to geographical coordinates: ArcGIS, Geonames and Google. These geocoding services rely on both historical placenames and fuzzy matching to find the coordinates and build on different databases for the matches. Additionally, we adapted a Geonames dump to find coordinates for the names with a preference for locations in or near Estonia and locations with larger populations. If all the coordinates found were within 20km from each other, we considered it a correct match and relied on it. If the placename was matched with coordinates more distant from each other, indicating that the databases contained different priority entries with that name, we manually resolved the location. Based on these comparisons we iteratively constructed a list of rules and exceptions that we then applied in a rerun at the beginning of the workflow. As an additional control, we were able to rely on country-codes given in the MARC data format. This country-code list in MARC format has gradually been developed since its inception in 1967, with the last update in 2003. As a result, they don't fully match modern country borders, however, they can be well used to compare with the general area in question. These country-codes were given by the cataloguer demonstrating also some variation in itself. For placenames with conflicting coordinates we checked whether any of the coordinates were within the country marked in the code and preferred these coordinates if this was the case. Some books had several publication locations represented with a semicolon: in this case all placenames were processed separately. 

```{r get data,echo=F,warning=F,include=F}
suppressMessages(library(data.table))
suppressMessages(library(tidyverse))
suppressMessages(library(nanoparquet))
suppressMessages(library(scales))
options(scipen=9999)

works_parq <- nanoparquet::read_parquet("../data/curated/enb_all_books.parquet")
works <-  setDT(works_parq)
rm(works_parq)

#collect unique place names from places of publication, manufacture and distribution.
unlist_places_publication <- works[,.(place_unlist=unlist(str_split(str_replace(publication_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]
unlist_places_manufacture <- works[,.(place_unlist=unlist(str_split(str_replace(manufacturing_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]
unlist_places_distribution <-  works[,.(place_unlist=unlist(str_split(str_replace(original_distribution_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]

unlist_places <- unique(rbind(unlist_places_publication,unlist_places_manufacture,unlist_places_distribution)[!is.na(place_unlist)])
```


```{r harmonize rulebased,echo=F,warning=F,include=F}
#6485 books have 2 or more locations
#unlist_places_publication[,.(.N,place_unlist),id][N>1][,uniqueN(id)]


works1 <- merge(works,unlist_places, by="id")
#all_places <- works1[,.N,.(place_unlist)]
all_places <- unique(works1[,.(place_unlist)])
all_places[,place_for_edits:=place_unlist]

all_places[str_detect(place_for_edits,"^\\[.*\\]$"),place_for_edits:=str_replace(place_for_edits, "[\\)\\]]+$", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "^[\\(\\[]+", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "[\\(\\[].*$", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "[\\(\\[\\)\\]]", "")]
all_places[,place_for_edits:=trimws(place_for_edits)]

places_rules <- fread("../config/places/places_rules.tsv",sep="\t",quote="",strip.white=F)
places_rules[,find_this:=str_replace_all(find_this,"\\\\{2}","\\\\")]

regexes_replace1 <- places_rules[order==1&type=="regex_replace"]
#merged[str_detect(standardizing_name,"noor eesti")]
#i <- 273
#merged[str_detect(standardizing_name,regexes[i,find_this])]
#i <- 2
#all_places[str_detect(place_for_edits,regexes_replace1[i,find_this])]
for (i in 1:nrow(regexes_replace1)){
  all_places[str_detect(place_for_edits,regexes_replace1[i,find_this]),replace_with:=regexes_replace1[i,replace_with]]
  all_places[str_detect(place_for_edits,regexes_replace1[i,find_this]),type:="regex_replace"]
  all_places[!is.na(replace_with),place_for_edits:=replace_with]
  all_places[,place_for_edits:=trimws(place_for_edits)]
}

regexes_partial1 <- places_rules[type=="regex_partial"&order==1]
#merged[str_detect(standardizing_name,"noor eesti")]
#

#i <- 1
#str_detect("[etc.]",regexes_partial1[i,find_this])
#all_places[str_detect(place_for_edits,regexes_partial1[i,find_this])]
for (i in 1:nrow(regexes_partial1)){
  all_places[str_detect(place_for_edits,regexes_partial1[i,find_this]),replace_with:=str_replace(place_for_edits,regexes_partial1[i,find_this],regexes_partial1[i,replace_with])]
  
  #all_places[str_detect(all_places,regexes_partial1[i,find_this]),type:="regex_partial"]
  all_places[!is.na(replace_with),place_for_edits:=replace_with]
  all_places[,place_for_edits:=trimws(place_for_edits)]
}
all_places[,replace_with:=NULL]

all_places_updated <- merge(all_places,places_rules[order==2],by.x="place_for_edits",by.y="find_this",all.x=T)[is.na(replace_with),match:=place_for_edits][!is.na(replace_with),match:=replace_with][,.(place_unlist,place_for_edits=match)]


#checking the results of this harmonization
works2 <- merge(works1,all_places_updated,by="place_unlist")

#split 110k books by this point
works2[,.(id,publication_place,place_unlist,place_for_edits)][publication_place!=place_unlist][,uniqueN(id)]
#changed 9k values by this point
#works2[,.(publication_place,manufacturing_place,original_distribution_place,place_unlist,place_for_edits)][place_unlist!=place_for_edits]

places <- works2[,.N,.(place_for_edits)][order(place_for_edits)]

exact_rules <- places_rules[order%in%c(3,4)&type=="exact"]
merged <- merge(places,exact_rules,by.x="place_for_edits",by.y="find_this",all.x=T)
merged[type=="exact",place_for_edits2:=replace_with]

#12459 sellega veidi paremaks

regexes <- places_rules[order%in%c(3,4)&type=="regex_replace"]
#merged[str_detect(standardizing_name,"noor eesti")]
#i <- 273
#merged[str_detect(standardizing_name,regexes[i,find_this])]
for (i in 1:nrow(regexes)){
  merged[str_detect(place_for_edits,regexes[i,find_this]),replace_with:=regexes[i,replace_with]]
  merged[str_detect(place_for_edits,regexes[i,find_this]),type:="regex_replace"]
  merged[!is.na(replace_with),place_for_edits2:=replace_with]
  merged[,place_for_edits2:=trimws(place_for_edits2)]
}
merged[!is.na(place_for_edits2)]
merged[is.na(place_for_edits2),place_for_edits2:=place_for_edits]

merged[nchar(place_for_edits2)<2,place_for_edits2:=""]

works3 <- merge(works2,unique(merged[,.(place_for_edits,place_for_edits2)]),by="place_for_edits",all.x=T)

#changed 4.8k additional entries here
#works3[,.(publication_place,place_unlist,place_for_edits,place_for_edits2)][place_for_edits!=place_for_edits2]
#works3[place_for_edits=="",uniqueN(id)]
works3[,n_places_work:=.N,id]
#works3[n_places_work>1]

works3[order(-place_for_edits),.SD[1],id][place_for_edits=="",uniqueN(id)]

places <- works3[,.N,.(place_for_edits=place_for_edits2)][order(place_for_edits)]
overview_after_rules <- works3[,.N,.(place_for_edits=place_for_edits2,place_unlist)][order(place_for_edits)]
fwrite(overview_after_rules,"~/Downloads/overview_after_rules_onefile.tsv",sep="\t")

```


```{r add geoinfo,echo=F,warning=F,include=F}
#fwrite(places2,"data/places4.tsv",sep="\t")

cities <- fread("../config/geoinfo/places_geonames_dump_cities500.txt")
cities2 <- cities[,.(variant_names=unlist(str_split(V4,","))),by=names(cities)]
biggest_pop_variant <- cities2[order(-V15),.SD[1],variant_names]
rm(cities,cities2)

places_w_geonames_global <- merge(places, biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)
places_w_geonames_global[!is.na(V1),uniqueN(place_for_edits)]
places_w_geonames_global[is.na(V1),uniqueN(place_for_edits)]
ee_places <- fread("../config/geoinfo/places_geonames_dump_EE.txt")
ee_towns <- ee_places[V7=="P"]
ee_towns[duplicated(V2)] #7k names, 1.5k duplicated
ee_towns2 <- ee_towns[,.(variant_names=unlist(str_split(V4,","))),by=names(ee_towns)]
ee_towns2[,uniqueN(variant_names)]
ee_biggest_pop_variant <- ee_towns2[variant_names=="",variant_names:=V2][order(-V15),.SD[1],variant_names]
rm(ee_places,ee_towns,ee_towns2)

#Do all the names with ee too. E.g. Kabala vs global Kabala.
places_w_geonames_ee <- merge(places[,.(place_for_edits,N)], ee_biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)
places_w_geonames_ee[is.na(V1)]
places_w_geonames_ee[!is.na(V1),uniqueN(place_for_edits)]
places_w_geonames_ee[is.na(V1),uniqueN(place_for_edits)]

check_missing <- places_w_geonames_ee[is.na(V1),.(place_for_edits,N)]


#places_unsolved_w_arcgis <- places_w_geonames_ee[is.na(V1)][,c("arcgis_name", "col2","col3"):=tidygeocoder::geo(address = place_for_edits, method = "arcgis")]
#places_w_geonames_global_and_arcgis <- merge(places_unsolved_w_arcgis,places_w_geonames_global[,.(publication_place,publication_place_control)],by="publication_place")
#places_w_geonames_global_and_arcgis <- places_w_geonames_global_and_arcgis[place_for_edits!="s. l"][place_for_edits!="S. l"]
#fwrite(places_w_geonames_global_and_arcgis[,.(place_for_edits,publication_place,publication_place_control,N,arcgis_name,col2,col3)],"data/places_arcgis2.tsv",sep="\t")


# places_all_w_arcgis <- data.table()
# for (i in 0:floor((nrow(places))/100)){
#   print(paste0("Getting rows ", ((100*i)+1)," ... ",min((100*(i+1)),nrow(places))))
#   places_w_arcgis <- places[((100*i)+1):min((100*(i+1)),nrow(places)),c("arcgis_name", "col2","col3"):=tidygeocoder::geo(address = place_for_edits, method = "arcgis")]
#   #places_all_w_arcgis <- rbind(places_all_w_arcgis, places_w_arcgis,fill=T)
#  # fwrite(places_w_arcgis[,.(location=place_for_edits,N,lon=col2,lat=col3)],"data/places_arcgis_all.tsv",sep="\t")
# }
#fwrite(places_all_w_arcgis[,.(location=place_for_edits,N,lon=col2,lat=col3)],"data/places_arcgis_all.tsv",sep="\t")
places_w_arcgis <- fread("../config/geoinfo/places_arcgis_api.tsv",sep="\t")

#[,-c("publication_place")])
#unique(

places_w_geonames_ee[,.N,place_for_edits][N>1]

merge_method <- rbind(places_w_geonames_global[!is.na(V1)],places_w_geonames_ee[!is.na(V1)],fill=T)
merge_method[,duplicates:=.N,.(place_for_edits)]
check_duplicates <-merge_method[duplicates>1][order(place_for_edits)]
check_duplicates[,exact:=ifelse(length(unique(V1))==1,yes=T,no=F),by=place_for_edits]

for_sorting <- check_duplicates[exact!=T]
for_sorting[V9=="EE",count_ee:=.N,place_for_edits]
prefer_in_ee <- for_sorting[count_ee==1]
both_ee <- for_sorting[count_ee==2]
prefer_higher_pop_in_ee <- both_ee[order(-V15),.SD[1],place_for_edits]

keep <- rbind(merge_method[duplicates==1],check_duplicates[exact==T,.SD[1],place_for_edits],prefer_in_ee,prefer_higher_pop_in_ee,fill=T)


#all_methods <- rbind(places_w_geonames_global[!is.na(V1)],places_w_geonames_ee[!is.na(V1)],save2[,.(place_for_edits,publication_place,publication_place_control,N,V6=col2,V5=col3)],fill=T)

tagged1 <- fread("../config/geoinfo/places_geonames_api.tsv",sep="\t")[!is.na(lon)]
tagged2 <- fread("../config/geoinfo/places_google_api.tsv",sep="\t")[!is.na(lon)]

#name variants.
keep[,.N,V2][N>1]

#works3[is.na(place_for_edits2)&!is.na(place_for_edits)]
places2 <- works3[,.N,.(publication_place_control,place_for_edits=place_for_edits2,place_unlist)][order(place_for_edits)]


combine_data <- merge(places2[,.(place_for_edits,publication_place_control,N)],keep[,-c("N")],by="place_for_edits",all.x=T)
compare_methods <- merge(combine_data,tagged1[,.(location,geonames_lat=lat,geonames_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods2 <- merge(compare_methods,tagged2[,.(location,google_lat=lat,google_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- merge(compare_methods2,places_w_arcgis[,.(location,arcgis_lon=lat,arcgis_lat=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- compare_methods3[order(-N)][!is.na(place_for_edits)]
compare_methods3[,uniqueN(place_for_edits)]
compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(V5),uniqueN(place_for_edits)] 
compare_methods3[!is.na(publication_place_control),uniqueN(place_for_edits)] 




compare_methods3[!is.na(geonames_lat),harmonize_geonames:=place_for_edits[1],.(geonames_lat,geonames_lon)]
compare_methods3[!is.na(google_lat),harmonize_google:=place_for_edits[1],.(google_lat,google_lon)]
compare_methods3[!is.na(arcgis_lat),harmonize_arcgis:=place_for_edits[1],.(arcgis_lat,arcgis_lon)]


compare_methods3[,lat_diff:=max(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)-min(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T),by=place_for_edits][,lon_diff:=max(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)-min(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T),by=place_for_edits]
compare_methods3[,diff:=lat_diff+lon_diff]
#compare_methods3[place_for_edits=="Adelaide"][,max(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,min(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,max(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,min(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)]

#3776 u. on enamvähem samas kohas.
compare_methods3[diff<0.2,uniqueN(place_for_edits)]
compare_methods3[diff>=0.2,uniqueN(place_for_edits)]

# 1149 tuleks käsitsi asuplace_for_edits ära lahendada, et mis on õige ja mis maha kriipsutada. seda enne harmoniseerimist.
check <- melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff"))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)]
unique(check)

melt_coords <- dcast(unique(melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff")))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)], place_for_edits + publication_place_control + N + diff + Source + geonames_name ~ lon_lat)
coord_options <- melt_coords[!is.na(lat)]
big_diffs <- coord_options[diff>0.2]
est_places <- big_diffs[lat<60&lat>57&lon>21&lon<29][,est_loc:=T]
est_places[,uniqueN(place_for_edits)]
#check_span <- big_diffs[publication_place_control=="er"]
#est_places <- big_diffs[,.(est_loc=any(lat<60&lat>56lon>24&lon<26)),by=place_for_edits]
with_est_loc <- merge(big_diffs,unique(est_places[,.(place_for_edits,est_loc)]),by="place_for_edits",all.x=T)

est_preferred_where_possible <- with_est_loc[est_loc==F|(lat<60&lat>56&lon>21&lon<29)]
est_preferred_where_possible[,newdiff:=(max(lat)-min(lat))+(max(lon)-min(lon)),by=place_for_edits]

loc_not_in_est <- with_est_loc[is.na(est_loc)]
loc_not_in_est[,uniqueN(place_for_edits)]

compare_methods3[,any_lat:=V5][is.na(any_lat),any_lat:=geonames_lat][is.na(any_lat),any_lat:=google_lat][is.na(any_lat),any_lat:=arcgis_lat]
compare_methods3[,any_lon:=V6][is.na(any_lon),any_lon:=geonames_lon][is.na(any_lon),any_lon:=google_lon][is.na(any_lon),any_lon:=arcgis_lon]

matching_places <- compare_methods3[diff<0.2][,.(place_for_edits,geonames_name=V2,N,lat=any_lat,lon=any_lon)][place_for_edits!=""]
est_preferred_places <- est_preferred_where_possible[newdiff<0.2][!is.na(lat),.(lat=lat[1],lon=lon[1]),.(place_for_edits,geonames_name,N)]
need_resolving <- rbind(loc_not_in_est,est_preferred_where_possible[newdiff>0.2],fill=T)
#fwrite(need_resolving,"data/need_resolving_geo.tsv",sep="\t") #find match in authority file.

countries <- fread("../config/geoinfo/marc_country_codes.tsv",sep="\t")[,code:=trimws(code)]
need_resolving2 <- merge(need_resolving[,publication_place_control:=trimws(publication_place_control)],countries,by.x="publication_place_control",by.y="code",all.x=T)
need_resolving2[,uniqueN(place_for_edits)]

library(maps)
need_resolving3<-need_resolving2[,country_revgeo:=map.where(database="world", lon, lat)]
#fwrite(need_resolving3,"data/need_resolving_geo.tsv",sep="\t") #find match in authority file.

#countries2 <- unique(need_resolving2[,.(country)])[,c("arcgis_name", "col2","col3"):=tidygeocoder::geo(country = country, method = "osm")]
#tidygeocoder::geo(country = "Russia", method = "osm")
#tidygeocoder::geo(country = "Georgia", method = "osm")
#tidygeocoder::geo(county = "Puerto Rico", method = "osm")
#check <- merge(need_resolving2,countries2,by="country")
#check[,diff_lat:=lat-col2][,diff_lon:=lon-col3][,diff_ctry:=diff_lat+diff_lon]
#check[order(diff_ctry),.SD[1],by=place_for_edits]

resolved_manually <- fread("../config/places/resolved_manually_geo.tsv",sep="\t")[,resolved:=T]
#just take the first one, they are mostly compatible. errors can be found later.
resolved_manually_for_merge <- resolved_manually[,.SD[1],.(place_for_edits,publication_place_control)]

need_resolving4 <- need_resolving3[!resolved_manually_for_merge,on=c("place_for_edits","publication_place_control")][!str_detect(place_for_edits,"S. ?l|Б.м.")]
add_some_estonian_locations <- need_resolving4[publication_place_control=="er"&str_detect(country_revgeo,"Estonia")][,.SD[1],.(place_for_edits,publication_place_control)]
need_resolving5 <- need_resolving4[!add_some_estonian_locations,on=c("place_for_edits","publication_place_control")]
  
#fwrite(need_resolving4,"data/need_resolving_geo2.tsv",sep="\t") #find match in authority file.
# save these ones together too here.

#if population data exists then take the larger location.
add_some_estonian_locations <- need_resolving4[publication_place_control=="er"&country_revgeo=="Estonia"][,.SD[1],.(place_for_edits,publication_place_control)]
need_resolving5 <- need_resolving4[!add_some_estonian_locations,on=c("place_for_edits","publication_place_control")]
  
#fwrite(need_resolving4,"data/need_resolving_geo2.tsv",sep="\t") #find match in authority file.
# save these ones together too here.
```


```{r harmonize with geoinfo,echo=F,warning=F,include=F}
#rbind(authority_input,resolved_manually_for_merge)
authority_input <- rbind(matching_places,est_preferred_places,resolved_manually_for_merge,add_some_estonian_locations,fill=T)
authority_input[,lat:=round(round(lat,2)*5,1)/5][,lon:=round(round(lon,2)*5,1)/5]
#authority_input <- authority_input[order(place_for_edits)]
authority_input[!str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=T][str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=F]
#authority_input[,.SD,by=latin][!is.na(place_for_edits)]
authority_input <- rbind(authority_input[latin==T],authority_input[latin==F])
authority_input[!is.na(geonames_name)&!(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=geonames_name]
authority_input[is.na(geonames_name)|(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=place_for_edits]
authority_input[!is.na(lat),harm_name:=place_for_edits2[1],.(lat,lon)]
#authority_input[is.na(place_for_edits2)]
#est_preferred_where_possible[publication_place_control!="er"]

diffs_more <- unique(authority_input[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
#nchar(prev_name)<nchar(harm_name)

subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]

authority_updated <- merge(unique(authority_input),subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]
diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]

diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]
```


```{r main outputs,echo=F,warning=F,include=F}
works4 <- merge(works3,unique(authority_updated[,.(harm_name,place_for_edits)]),by.x="place_for_edits2",by.y="place_for_edits",all.x=T)
works4_geo <- merge(works4,unique(authority_updated[!is.na(lat),.(harm_name,lat,lon)]),by.x="harm_name",by.y="harm_name",all.x=T)

tartu_variants <- works4[harm_name=="Tartu",.N,.(harm_name,place_unlist)][order(-N)]
tartu_variants[,.(sum(N))]
#fwrite(tartu_variants,"../reports/testsets/testset_tartu_variants.tsv",sep="\t")

harm_names <- unique(works4[,.(harm_name,place_for_edits,place_unlist)])
harm_locations <- unique(works4_geo[!duplicated(place_unlist)][!is.na(lat),.(harm_name,lat,lon)])

#est_preferred_where_possible[publication_place_control!="er"]

#sort(c("Ярославль","Jaroslav"))

places_harmonized_new <- harm_names[order(harm_name)][,.(place_original=place_unlist,place_harmonized=harm_name)][!duplicated(place_original)]
places_geotagged_new <- harm_locations[,.(place_harmonized=harm_name,lat,lon)]

fwrite(places_harmonized_new,"../config//places/places_harmonized_new.tsv",sep="\t")
fwrite(places_geotagged_new,"../config/places/places_geotagged_new.tsv",sep="\t")

# 
# places_harmonized_old <- fread("../config//places/places_harmonized.tsv",sep="\t")
# places_geotagged_old <- fread("../config//places/places_coordinates.tsv",sep="\t")
# 
# places_geotagged_old[,uniqueN(place_harmonized)]
# places_geotagged_new[,uniqueN(place_harmonized)]
# 
# 
# check <- merge(harm_names[order(harm_name)][,.(place_original=place_unlist,place_harmonized=harm_name)],places_harmonized_old,by="place_original",all=T)
# check2 <- check[place_harmonized.x!=place_harmonized.y]
# 
# check3 <- merge(check2,places_geotagged_new,by.x="place_harmonized.x",by.y="place_harmonized",all.x=T)
# check4 <- merge(check3,places_geotagged_old,by.x="place_harmonized.y",by.y="place_harmonized",all.x=T)

#fwrite(harm_names,"config/places/authority_input_places.tsv",sep="\t")
#fwrite(harm_locations,"data/authority_input_geo.tsv",sep="\t")


works_harmonized <- works4[,.(id,publication_place_original=place_unlist,publication_place_harmonized=harm_name)]
#fwrite(works_harmonized,"data/works_publication_place_harmonized.tsv",sep="\t")


#mälivere, käru viga. google või arcgis pani käru [raplamaa]-le mäliverega suht samad koordinaadid millegipärast. siis koordinaatide järgi läksid nad kokku. seda võis veel juhtuda teistelgi place_for_editsadel.

#31 places could be given an authority location.
need_resolving5[,uniqueN(place_for_edits)]
#2384
harm_names[,uniqueN(harm_name)]
harm_names[,uniqueN(place_unlist)]
harm_locations[,uniqueN(harm_name)]
works1[,uniqueN(place_unlist)]
#works3[place_unlist=="Tallinn?"]
harm_names2 <- unique(works4[,.(harm_name,place_for_edits)])

works4 <- merge(works3,harm_names2,by.x="place_for_edits2",by.y="place_for_edits",all.x=T)
works4_geo <- merge(works4,harm_locations,by.x="harm_name",by.y="harm_name",all.x=T)


more_exceptions <- works4_geo[is.na(lat)][,.N,.(place_for_edits)]
#fwrite(more_exceptions,"data/more_exceptions.tsv",sep="\t")
```


```{r testsets,echo=F,warning=F,include=F}
library("rnaturalearth")
library(sf)
world <- ne_countries(scale = "medium", returnclass = "sf")

top_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)]#[top_areas,on="publication_place_control"]
plot1 <- ggplot(data = world) +
    geom_sf(alpha=0.2) +
    geom_point(data = top_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
        shape = 20) +
    scale_size(trans="log10") +
    theme_bw()+
    labs(x="",y="")# +
    #coord_sf(xlim = c(-100, 78), ylim = c(24.5, 83), expand = FALSE)


works4_geo[!is.na(lat)][,.N,.(place_unlist)][,.N]
works4_geo[!is.na(lat)][,uniqueN(id)]
works4_geo[!is.na(lat)][,uniqueN(id)]/works4_geo[,uniqueN(id)]
works4_geo[is.na(lat)][,uniqueN(id)]
works4_geo[is.na(lat)][,.N,.(place_for_edits)]

sample_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)][,.(N=sum(N),publication_place_control=publication_place_control[1]),.(harm_name,lat,lon)][sample(.N,200)][order(-N)]#[top_areas,on="publication_place_control"]

#fwrite(sample_places3,"data/sample_places_for_checking.tsv",sep="\t")
plot1 <- ggplot(data = world) +
    geom_sf(alpha=0.2) +
    geom_point(data = sample_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
        shape = 20) +
    scale_size(trans="log10") +
    theme_bw()+
    labs(x="",y="")


library(htmlwidgets)
library(plotly)
#saveWidget(ggplotly(plot1), file=paste0( getwd(), "/sample_locations_on_map.html"))

works4[place_unlist=="Palermo"]
works4[place_unlist=="Novi Sad"]

works[id=="b13096497"]

checked_places <- fread("../reports/testsets/testset_places_checked.tsv",sep="\t")
checked_places[,.N/2,true_place][true_place=="T"][,V1]

works4[is.na(harm_name)][,uniqueN(id)]
works4[is.na(harm_name)&!is.na(place_for_edits)][,uniqueN(id)]

works4[harm_name==place_unlist][,uniqueN(id)]
#59225
works4[harm_name!=place_unlist][,uniqueN(id)]
works4[harm_name!=place_unlist][,uniqueN(id)]/nrow(works4)
works4_geo[publication_place=="Tapal"]

works_harmonized <- works4[,.(id,place=place_unlist,harm_name)]
#fwrite(works_harmonized,"data/works_harmonized_place.tsv",sep="\t")

works4[is.na(harm_name)&!is.na(place_for_edits)]
mappings <- unique(works4[,.(harm_name,place_unlist)][!duplicated(place_unlist)])[!is.na(harm_name)]



#4707 varianti.
#2368 harmoniseeritud nime.
#2368 asuplace_for_editsa
```


```{r interactive plots,echo=F,warning=F,include=F}
library(networkD3)

# create a dataset:
data <- data_frame(mappings[,.(from=harm_name,to=place_unlist)])

# Plot
p <- simpleNetwork(data, height="1000px", width="1000px",zoom=T)

# p

#save the widget
#
library(htmlwidgets)
saveWidget(p, file=paste0( getwd(), "/../reports/interactive_plots/places_harmonized_network.html"))


# Tartoh
# Kuresaare [i.e. Kuressaare]


more_exceptions <- works4_geo[is.na(lat)][,.N,.(place_for_edits)]
#fwrite(more_exceptions,"data/more_exceptions.tsv",sep="\t")

library("rnaturalearth")
library(sf)
world <- ne_countries(scale = "medium", returnclass = "sf")

top_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)]#[top_areas,on="publication_place_control"]
plot1 <- ggplot(data = world) +
  geom_sf(alpha=0.2) +
  geom_point(data = top_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
             shape = 20) +
  scale_size(trans="log10") +
  theme_bw()+
  labs(x="",y="")# +
#coord_sf(xlim = c(-100, 78), ylim = c(24.5, 83), expand = FALSE)




library(htmlwidgets)
library(plotly)
saveWidget(ggplotly(plot1), file=paste0( getwd(), "/../reports/interactive_plots/places_on_map.html"))

all_places[,uniqueN(place_unlist)]
all_places[,uniqueN(place_for_edits)]
mappings[,uniqueN(place_unlist)]
mappings[,uniqueN(harm_name)]

nrow(places_rules)


compare_methods3[,uniqueN(place_for_edits)]
compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(V5),uniqueN(place_for_edits)] 
compare_methods3[!is.na(publication_place_control),uniqueN(place_for_edits)] 

compare_methods3[diff<0.2,uniqueN(place_for_edits)]
compare_methods3[diff>=0.2,uniqueN(place_for_edits)]
scales::comma(works4_geo[!is.na(lat)][,uniqueN(id)])
```


In all, there were `r all_places[,uniqueN(place_unlist)]` place name variants in the dataset. We harmonized  `r mappings[,uniqueN(place_unlist)]` of these variants to match `r mappings[,uniqueN(harm_name)]` harmonized placenames, while applying `r nrow(places_rules)` rules and exceptions based on regular expressions and variant matches to clean the dataset. After applying these methods, there were a total of `r all_places[,uniqueN(place_for_edits)]` unique placenames in the dataset. `r compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)]` of them could be geocoded with ArcGIS, `compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)] ` of them with Google, `r compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)] ` of them with Geonames, and `r compare_methods3[!is.na(V5),uniqueN(place_for_edits)] ` in working with a local copy of Geonames with historical placenames. `r compare_methods3[diff<0.2,uniqueN(place_for_edits)]`had all the acquired coordinates within approximately 20 km from within each other and were considered non-conflicting matches. `r compare_methods3[diff>=0.2,uniqueN(place_for_edits)]` placenames showed bigger differences. In this case, if any of the placenames were in or near Estonia, they were preferred, providing a solution for `r est_places[,uniqueN(place_for_edits)]` places with no conflicts on coordinates in Estonia. Of these, `r need_resolving2[,uniqueN(place_for_edits)]` conflicting placenames were then resolved manually, in preferring the geographic match that was in or nearby the area marked in the MARC data format. `r need_resolving5[,uniqueN(place_for_edits)]` placenames could not be reasonably resolved to particular coordinates with this method. Through the process `r works4[harm_name!=place_unlist][,uniqueN(id)]` books were given a new name that was either cleaned up or grouped together with other name variants.

As a result, `r works4_geo[!is.na(lat)][,.N,.(place_unlist)][,.N]` unique placenames in the dataset have been given coordinates, providing a coordinate for `r scales::percent(works4_geo[!is.na(lat)][,uniqueN(id)]/works4_geo[,uniqueN(id)],3)` of the books (n = `r scales::comma(works4_geo[!is.na(lat)][,uniqueN(id)])`), `r need_resolving5[,uniqueN(place_for_edits)]` placenames there have not been resolved, and `r works3[order(-place_for_edits),.SD[1],id][place_for_edits=="",uniqueN(id)]` books in the dataset do not include information on the place of publication. The harmonization and linking of the places of publication is likely to contain some errors, but a manual verification showed the results to be reasonably accurate ( `r checked_places[,.N/2,true_place][true_place=="T"][,V1]` % of 200 randomly selected placenames showed accurate coordinates).  

