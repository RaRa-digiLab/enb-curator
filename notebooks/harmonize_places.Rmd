---
title: "Harmonizing place names"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
    self_contained: no
editor_options:
  chunk_output_type: console
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../reports"
    )
  })
---

# Harmonizing place names

Last data file 08-10-2024, last updated processing 16-01-2025.

```{r setup,echo=F}
#knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

Catalogue data entry practices typically aim for precision. This means that publisher names and locations are entered as originally printed, where substantial variation in presentation can be present. In a long-spanning dataset this can include historical variants of placenames (e.g. Dorpat, Tarbatu, Derpt, Jurjev, Tartu), grammar (Tartus, Tartun, In Tartu), localized versions of placenames (Tartto, Tērbata, Тарту), historical spelling variants (Tarto-linan, Tartolinan, Iourieff, Jujew). The entries often have additional specifications too, including other variants (Tartu – Dorpat, Youriev (Tartu)), wider geographical areas (Tartu (Tartumaa)), markers for other places of publication (etc, jne), publisher name placeholders (Tartu:[s.n]) and even several placenames sometimes (Tartu [i.e. Torino], Tallinn [p.o. Tartu]). These variants present a challenge for data analysis, as the data field needs to be harmonized in aggregating all books that were, for example, published in Tartu. 

Data harmonization is a common task for adapting catalogue data for computational analysis, where a general framework suitable for any dataset is difficult to offer. The historical variants and spelling variants cannot effectively be derived from the placenames, with even predictable variants relying on language-specific patterns. Additionally, the extent of variation within a particular dataset can be difficult to predict and will inevitably rely on custom effort in each case. There are some general patterns: for example, sometimes the placenames include "Printed in", or "etc", sometimes they rely on local grammar, books printed in another language are likely to include a localized version of the placename, however even their adaptation is language and case dependent. 

To harmonize the placenames, we built a general workflow that relies on external geographical databases that contain the relevant placenames as well as their historical variants. To do this, we first removed the grammatical markers from the placenames where possible. In Estonian case, the frequent forms here were placenames ending with an s, which were extracted from the set and manually annotated for nominative variants of the names. We then used several geocoding services to link the placenames to geographical coordinates: ArcGIS, Geonames and Google. These geocoding services rely on both historical placenames and fuzzy matching to find the coordinates and build on different databases for the matches. Additionally, we adapted a Geonames dump to find coordinates for the names with a preference for locations in or near Estonia and locations with larger populations. If all the coordinates found were within 20km from each other, we considered it a correct match and relied on it. If the placename was matched with coordinates more distant from each other, indicating that the databases contained different priority entries with that name, we manually resolved the location. Based on these comparisons we iteratively constructed a list of rules and exceptions that we then applied in a rerun at the beginning of the workflow. As an additional control, we were able to rely on country-codes given in the MARC data format. This country-code list in MARC format has gradually been developed since its inception in 1967, with the last update in 2003. As a result, they don't fully match modern country borders, however, they can be well used to compare with the general area in question. These country-codes were given by the cataloguer demonstrating also some variation in itself. For placenames with conflicting coordinates we checked whether any of the coordinates were within the country marked in the code and preferred these coordinates if this was the case. Some books had several publication locations represented with a semicolon: in this case all placenames were processed separately. 

```{r get data,echo=F,warning=F,include=F}
# Read the packages
suppressMessages(library(data.table))
suppressMessages(library(tidyverse))
suppressMessages(library(nanoparquet))
suppressMessages(library(scales))
options(scipen=9999)

# Read the parquet file and convert it into data.table. Remove the parquet instance.
works_parq <- nanoparquet::read_parquet("../data/curated/enb_all_books.parquet")
works <-  setDT(works_parq)
rm(works_parq)

#Collect unique place names from places of publication, manufacture and distribution.
unlist_places_publication <- works[,.(place_unlist=unlist(str_split(str_replace(publication_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]
unlist_places_manufacture <- works[,.(place_unlist=unlist(str_split(str_replace(manufacturing_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]
unlist_places_distribution <-  works[,.(place_unlist=unlist(str_split(str_replace(original_distribution_place, " und | u\\. | & ", " ; "),";"))),id][,place_unlist:=trimws(place_unlist)]

# Combine them for the list of all variant place names.
unlist_places <- unique(rbind(unlist_places_publication,unlist_places_manufacture,unlist_places_distribution)[!is.na(place_unlist)])
```


```{r harmonize rulebased,echo=F,warning=F,include=F}

# Make a combined table combining the placenames with other data if needed for extra statistics.
works1 <- merge(works,unlist_places, by="id")
# E.g. to include frequency counts.
#all_places <- works1[,.N,.(place_unlist)]

# But for simple conversion they are not needed so we proceed with the unique places without the work id.
all_places <- unique(works1[,.(place_unlist)])
all_places[,place_for_edits:=place_unlist]

# Make initial changes to the place name.
all_places[str_detect(place_for_edits,"^\\[.*\\]$"),place_for_edits:=str_replace(place_for_edits, "[\\)\\]]+$", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "^[\\(\\[]+", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "[\\(\\[].*$", "")]
all_places[,place_for_edits:=str_replace(place_for_edits, "[\\(\\[\\)\\]]", "")]
all_places[,place_for_edits:=trimws(place_for_edits)]

# Read the harmonization rules
places_rules <- fread("../config/places/places_harmonize_rules.tsv",sep="\t",quote="",strip.white=F)
places_rules[,find_this:=str_replace_all(find_this,"\\\\{2}","\\\\")]

# Get tbe first set of rules, ordered one, with the type regex_replace
regexes_replace1 <- places_rules[order==1&type=="regex_replace"]

# Regex_replace here means that if a matching substring is found, the whole string is replaced by another.
# For example replacing s.l (no location) with NA.
for (i in 1:nrow(regexes_replace1)){
  all_places[str_detect(place_for_edits,regexes_replace1[i,find_this]),replace_with:=regexes_replace1[i,replace_with]]
  all_places[str_detect(place_for_edits,regexes_replace1[i,find_this]),type:="regex_replace"]
  all_places[!is.na(replace_with),place_for_edits:=replace_with]
  all_places[,place_for_edits:=trimws(place_for_edits)]
}

# Next the partial regexes are used.
regexes_partial1 <- places_rules[type=="regex_partial"&order==1]

# Here only the matching part of the substring will be replaced.
# E.g. when removing a question mark from the end of a place name.
for (i in 1:nrow(regexes_partial1)){
  all_places[str_detect(place_for_edits,regexes_partial1[i,find_this]),replace_with:=str_replace(place_for_edits,regexes_partial1[i,find_this],regexes_partial1[i,replace_with])]
  all_places[!is.na(replace_with),place_for_edits:=replace_with]
  all_places[,place_for_edits:=trimws(place_for_edits)]
}
all_places[,replace_with:=NULL]

# Third a set of rules (order==2) is used to change some exact place names into others.
all_places_updated <- merge(all_places,places_rules[order==2],by.x="place_for_edits",by.y="find_this",all.x=T)[is.na(replace_with),match:=place_for_edits][!is.na(replace_with),match:=replace_with][,.(place_unlist,place_for_edits=match)]


# We can check the results of this harmonization
works2 <- merge(works1,all_places_updated,by="place_unlist")

#We have changed 24k values by this point
works2[,.(publication_place,manufacturing_place,original_distribution_place,place_unlist,place_for_edits)][place_unlist!=place_for_edits]

# We continue with applying the rules
places <- works2[,.N,.(place_for_edits)][order(place_for_edits)]

# Some more exact replacements here
exact_rules <- places_rules[order%in%c(3,4)&type=="exact"]
merged <- merge(places,exact_rules,by.x="place_for_edits",by.y="find_this",all.x=T)
merged[type=="exact",place_for_edits2:=replace_with]

# And some more regex replacements here.
regexes <- places_rules[order%in%c(3,4)&type=="regex_replace"]
for (i in 1:nrow(regexes)){
  merged[str_detect(place_for_edits,regexes[i,find_this]),replace_with:=regexes[i,replace_with]]
  merged[str_detect(place_for_edits,regexes[i,find_this]),type:="regex_replace"]
  merged[!is.na(replace_with),place_for_edits2:=replace_with]
  merged[,place_for_edits2:=trimws(place_for_edits2)]
}

# check the changes done in this section.
merged[!is.na(place_for_edits2)]
merged[is.na(place_for_edits2),place_for_edits2:=place_for_edits]

# Another rule here - if the string is just one character long, make it into an empty string.
merged[nchar(place_for_edits2)<2,place_for_edits2:=""]

# We add also these values to the main dataframe.
works3 <- merge(works2,unique(merged[,.(place_for_edits,place_for_edits2)]),by="place_for_edits",all.x=T)

#We have changed 13k additional entries here
works3[,.(publication_place,place_unlist,place_for_edits,place_for_edits2)][place_for_edits!=place_for_edits2]

# So far 1222 place names are marked as empty place names (this includes places of publication, manufacture or distribution).
works3[order(-place_for_edits),.SD[1],id][place_for_edits=="",uniqueN(id)]

# We can make another list of place names, this time counting the harmonized versions.
places <- works3[,.N,.(place_for_edits=place_for_edits2)][order(place_for_edits)]


# To get an overview of the results so far, we write the mappings between harmonized names based on the rules with the place name variants into a file here.
overview_after_rules <- works3[,.N,.(place_for_edits=place_for_edits2,place_unlist)][order(place_for_edits)]
fwrite(overview_after_rules,"../reports/places_overview_rulebased harmonization.tsv",sep="\t")

```


```{r add geoinfo,echo=F,warning=F,include=F}

# We now add geographical information to the linked names.
# First we take the geonames dump with cities bigger than 500 inhabitants.
cities <- fread("../config/geoinfo/places_geonames_dump_cities500.txt")
# We take all the variant names of these cities
cities2 <- cities[,.(variant_names=unlist(str_split(V4,","))),by=names(cities)]

# And for overlapping variants we take the city with the biggest population
biggest_pop_variant <- cities2[order(-V15),.SD[1],variant_names]
# As these are large datasets, we remove them after they are not needed.
rm(cities,cities2)

# This is the first layer of geocoding, based on the global dump of geonames cities.
places_w_geonames_global <- merge(places, biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)

# A total of 1744 could not be joined, but 2133 could from the 3877 place names in the list.
places_w_geonames_global[!is.na(V1),uniqueN(place_for_edits)]
places_w_geonames_global[is.na(V1),uniqueN(place_for_edits)]

# We also add the towns located in Estonia. This includes also smaller places.
ee_places <- fread("../config/geoinfo/places_geonames_dump_EE.txt")
ee_towns <- ee_places[V7=="P"]

# There are 1500 duplicate names
ee_towns[duplicated(V2)] #7k names, 1.5k duplicated
# Take the variant names.
ee_towns2 <- ee_towns[,.(variant_names=unlist(str_split(V4,","))),by=names(ee_towns)]
ee_towns2[,uniqueN(variant_names)]
# And take the location with the biggest population for each variant name.
ee_biggest_pop_variant <- ee_towns2[variant_names=="",variant_names:=V2][order(-V15),.SD[1],variant_names]
rm(ee_places,ee_towns,ee_towns2)

# Join the table with Estonian names too - even the ones that were linked with global names. E.g. Kabala is a place in Estonia and also a larger city outside of it Kabala.
places_w_geonames_ee <- merge(places[,.(place_for_edits,N)], ee_biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)
places_w_geonames_ee[is.na(V1)]
#2864 names were joined, 1013 were not joined.
places_w_geonames_ee[!is.na(V1),uniqueN(place_for_edits)]
places_w_geonames_ee[is.na(V1),uniqueN(place_for_edits)]

# Check the names that were missing.
check_missing <- places_w_geonames_ee[is.na(V1),.(place_for_edits,N)]


# We can get geographic data from more sources. Here, we access arcgis data through their api via the tidygeocoder package in R.
# places_all_w_arcgis <- data.table()
# for (i in 0:floor((nrow(places))/100)){
#   print(paste0("Getting rows ", ((100*i)+1)," ... ",min((100*(i+1)),nrow(places))))
#   places_w_arcgis <- places[((100*i)+1):min((100*(i+1)),nrow(places)),c("arcgis_name", "col2","col3"):=tidygeocoder::geo(address = place_for_edits, method = "arcgis")]
#   #places_all_w_arcgis <- rbind(places_all_w_arcgis, places_w_arcgis,fill=T)
#  # fwrite(places_w_arcgis[,.(location=place_for_edits,N,lon=col2,lat=col3)],"data/places_arcgis_all.tsv",sep="\t")
# }
#fwrite(places_all_w_arcgis[,.(location=place_for_edits,N,lon=col2,lat=col3)],"data/places_arcgis_all.tsv",sep="\t")
# The results are stored into a file and reread for convenience.
places_w_arcgis <- fread("../config/geoinfo/places_arcgis_api.tsv",sep="\t")


# We can then start merging and comparing the different sources. First the two datasets in geonames.
merge_method <- rbind(places_w_geonames_global[!is.na(V1)],places_w_geonames_ee[!is.na(V1)],fill=T)
merge_method[,duplicates:=.N,.(place_for_edits)]
# We can find duplicates, place name variants in both datasets.
check_duplicates <-merge_method[duplicates>1][order(place_for_edits)]
check_duplicates[,exact:=ifelse(length(unique(V1))==1,yes=T,no=F),by=place_for_edits]

# If they are exact duplicates we can keep any one of them.
# If they are not exact duplicates we prefer the ones with the Estonian area code.
for_sorting <- check_duplicates[exact!=T]
for_sorting[V9=="EE",count_ee:=.N,place_for_edits]
prefer_in_ee <- for_sorting[count_ee==1]
both_ee <- for_sorting[count_ee==2]
# And then the name with the higher population.
prefer_higher_pop_in_ee <- both_ee[order(-V15),.SD[1],place_for_edits]

# We take all the sets together. Place names with no duplicate matches, the first row from each place name with exact duplicates, then the ones that have only one location in Estonia, and finally the ones that have the highest population in Estonia if there are several.
keep <- rbind(merge_method[duplicates==1],check_duplicates[exact==T,.SD[1],place_for_edits],prefer_in_ee,prefer_higher_pop_in_ee,fill=T)
#We still have several name variants here.
keep[,.N,V2][N>1]

# We can add two more datasets here, based on the geonames api and google api
tagged1 <- fread("../config/geoinfo/places_geonames_api.tsv",sep="\t")[!is.na(lon)]
tagged2 <- fread("../config/geoinfo/places_google_api.tsv",sep="\t")[!is.na(lon)]

# For a more complex merger, we can get the place names with the publication_place_control field from the bibliography. Note that this adds many more entries, since the same place name can carry several different marc codes, sometimes due to equivalence of the codes, sometimes due to errors in cataloguing.
places2 <- works3[,.N,.(publication_place_control,place_for_edits=place_for_edits2,place_unlist)][order(place_for_edits)]

# We can combine this dataset with the previous data sources above.
combine_data <- merge(places2[,.(place_for_edits,publication_place_control,N)],keep[,-c("N")],by="place_for_edits",all.x=T)
# We then also add geonames api and google api results here, as well as the arcgis api results.
compare_methods <- merge(combine_data,tagged1[,.(location,geonames_lat=lat,geonames_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods2 <- merge(compare_methods,tagged2[,.(location,google_lat=lat,google_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- merge(compare_methods2,places_w_arcgis[,.(location,arcgis_lon=lat,arcgis_lat=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- compare_methods3[order(-N)][!is.na(place_for_edits)]

# This gives us a table with a number of linked place names.
compare_methods3[,uniqueN(place_for_edits)]
# We can see in turn how many of the names were linked with geonames api, google api, arcgis api, geonames dump, and the bibliographic marc code in publication_place_control.
compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(V5),uniqueN(place_for_edits)] 
compare_methods3[!is.na(publication_place_control),uniqueN(place_for_edits)] 



# We can try to harmonize placenames based on locations within each of the databases. This lumps locations that have different names but identical locations together into one type. We don't use this for the workflow.
compare_methods3[!is.na(geonames_lat),harmonize_geonames:=place_for_edits[1],.(geonames_lat,geonames_lon)]
compare_methods3[!is.na(google_lat),harmonize_google:=place_for_edits[1],.(google_lat,google_lon)]
compare_methods3[!is.na(arcgis_lat),harmonize_arcgis:=place_for_edits[1],.(arcgis_lat,arcgis_lon)]

# We then calculate the differences between the locations in each of the linked datasets (if available)
compare_methods3[,lat_diff:=max(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)-min(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T),by=place_for_edits][,lon_diff:=max(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)-min(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T),by=place_for_edits]
compare_methods3[,diff:=lat_diff+lon_diff]
# For example, here are the scores for Adelaide that had very similar but not identical mappings across the datasets
#compare_methods3[place_for_edits=="Adelaide"][,.(diff)]
#compare_methods3[place_for_edits=="Adelaide"][,max(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,min(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,max(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)]
#compare_methods3[place_for_edits=="Adelaide"][,min(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)]

# A bit more than 3000 are mostly in the same place.
compare_methods3[diff<0.2,uniqueN(place_for_edits)]
# A bit more than 500 are in different places.
compare_methods3[diff>=0.2,uniqueN(place_for_edits)]
# Note that these numbers may decrease if more rules are added to harmonise the previous section.

# We can checka the fields that have a difference of more than 20 km in locations for the same name. We can plot these locations with one value per row
check <- melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff"))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)]
unique(check)

# We can make a more specialized table keeping both latitude and longitude on one row. This can be used to disambiguate between different coordinates for a place.
melt_coords <- dcast(unique(melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff")))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)], place_for_edits + publication_place_control + N + diff + Source + geonames_name ~ lon_lat)

# We can exclude the rows with no coordinates
coord_options <- melt_coords[!is.na(lat)]

# Then there are a few different options. For example, if the coordinates have big differences, we can check if any of them are in the area of Estonia, and mark them for preference. The area of Estonia is marked by a latitude longitude box.
big_diffs <- coord_options[diff>0.2]
est_places <- big_diffs[lat<60&lat>57&lon>21&lon<29][,est_loc:=T]
est_places[,uniqueN(place_for_edits)]

# We can check if they are appropriate.
#check_span <- big_diffs[publication_place_control=="er"]
#est_places <- big_diffs[,.(est_loc=any(lat<60&lat>56lon>24&lon<26)),by=place_for_edits]
# And pick out the ones with at least one location in Estonia.
with_est_loc <- merge(big_diffs,unique(est_places[,.(place_for_edits,est_loc)]),by="place_for_edits",all.x=T)

# This will be one rule for preferring coordinates over others.
est_preferred_where_possible <- with_est_loc[est_loc==F|(lat<60&lat>56&lon>21&lon<29)]
est_preferred_where_possible[,newdiff:=(max(lat)-min(lat))+(max(lon)-min(lon)),by=place_for_edits]

# Then we have coordinates that don't have any options near Estonia
loc_not_in_est <- with_est_loc[is.na(est_loc)]
loc_not_in_est[,uniqueN(place_for_edits)]

# For locations where the different databases don't differ much, we can just pick one. Here we set up a hierarchy of locations, preferring geonames dump to geonames api to google api to arcgis api.
compare_methods3[,any_lat:=V5][is.na(any_lat),any_lat:=geonames_lat][is.na(any_lat),any_lat:=google_lat][is.na(any_lat),any_lat:=arcgis_lat]
compare_methods3[,any_lon:=V6][is.na(any_lon),any_lon:=geonames_lon][is.na(any_lon),any_lon:=google_lon][is.na(any_lon),any_lon:=arcgis_lon]

# And then take the subset of places where the coordinates matched
matching_places <- compare_methods3[diff<0.2][,.(place_for_edits,geonames_name=V2,N,lat=any_lat,lon=any_lon)][place_for_edits!=""]
est_preferred_places <- est_preferred_where_possible[newdiff<0.2][!is.na(lat),.(lat=lat[1],lon=lon[1]),.(place_for_edits,geonames_name,N)]

# Finally we have places that are not near Estonia or where also the Estonian locations have a big difference in location. These places still need to be resolved. We can create a file to have a look at the intermediate step.
need_resolving <- rbind(loc_not_in_est,est_preferred_where_possible[newdiff>0.2],fill=T)
#fwrite(need_resolving,"data/need_resolving_geo.tsv",sep="\t")

# We can then use the information in publication_place_control to find a preferred location, if that is available. Since these abbreviations are uncommon, we read in an extra table with the full names of these abbreviations.
countries <- fread("../config/geoinfo/marc_country_codes.tsv",sep="\t")[,code:=trimws(code)]
# We get another set of mappings to resolve, now we have help from the values in the bibliography.
need_resolving2 <- merge(need_resolving[,publication_place_control:=trimws(publication_place_control)],countries,by.x="publication_place_control",by.y="code",all.x=T)
# Here we have 382 harmonized placenames.
need_resolving2[,uniqueN(place_for_edits)]

# To resolve this, we can then use the latitude and longitude information to add a country name to variant locations for a place. We can then check if they match the expected location within the bibliography. (Bibliography also has mistakes in the publication_place_control).
library(maps)
need_resolving3<-need_resolving2[,country_revgeo:=map.where(database="world", lon, lat)]
# We store this for viewing. We can simply use the file to resolve the ambiguities by removing all lines that had conflicting or unlikely locations for the placenames.
#fwrite(need_resolving3,"data/need_resolving_geo.tsv",sep="\t") 

# We can read in the file of the manually resolved lines that have all been confirmed to be plausible. Some place names may still have several candidates.
resolved_manually <- fread("../config/geoinfo/resolved_manually_geo.tsv",sep="\t")[,resolved:=T]
# For convenience we can currently just take the first plausible location and use that for now.
resolved_manually_for_merge <- resolved_manually[,.SD[1],.(place_for_edits,publication_place_control)]

# We can then see which of the place names still don't have a positive plausible resolution.
need_resolving4 <- need_resolving3[!resolved_manually_for_merge,on=c("place_for_edits","publication_place_control")][!str_detect(place_for_edits,"S. ?l|Б.м.")]

# In case they had a location in Estonia and the publication_place_control in Estonia we can also from this way prefer the Estonian localities.
add_some_estonian_locations <- need_resolving4[publication_place_control=="er"&str_detect(country_revgeo,"Estonia")][,.SD[1],.(place_for_edits,publication_place_control)]

# We then have still some names that do not have a resultion from there either.
need_resolving5 <- need_resolving4[!add_some_estonian_locations,on=c("place_for_edits","publication_place_control")]

# Some locations may still need resolving.
#fwrite(need_resolving4,"data/need_resolving_geo2.tsv",sep="\t").
```


```{r harmonize with geoinfo,echo=F,warning=F,include=F}

# Based on these geographic mappings to harmonized names we can make an authority input to the pipeline. This includes then 1) place names where the geoinformation systems agreed in locations, 2) places that had exactly one Estonian location, 3) places that were resolved manually, 4) places with at least one Estonian location.

authority_input <- rbind(matching_places,est_preferred_places,resolved_manually_for_merge,add_some_estonian_locations,fill=T)

# We can then also harmonize on the basis of geographic location. If two placenames have similar locations (in roughly 20 km radius), then they can be lumped together as same or nearby localities.
# For generalizations we remove precision from latitude and longitude. Keeping a roughly 20 km precision.
authority_input[,lat:=round(round(lat,2)*5,1)/5][,lon:=round(round(lon,2)*5,1)/5]

# For harmonization a preferred name can be chosen.
# A number of Estonian locations were often spelled with Cyrillic at particular times in Estonian history. For uniformity, place names that do not include Cyrillic are here preferred.
authority_input[!str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=T][str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=F]
authority_input <- rbind(authority_input[latin==T],authority_input[latin==F])
# Then, if the locations are outside of Estonia geonames default names are preferred.
authority_input[!is.na(geonames_name)&!(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=geonames_name]
# If the locations are missing a geonames name or are located in Estonia, then the most common name from the bibliography is preferred.
authority_input[is.na(geonames_name)|(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=place_for_edits]
# Then, for each location with coordinates, we can give the first preferred name as the harmonized name for each location.
authority_input[!is.na(lat),harm_name:=place_for_edits2[1],.(lat,lon)]

# We can also compare if the locations that are alphabetically next to each other may be still close on the map.
diffs_more <- unique(authority_input[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]

# Here we find almost 100 such locations, for example Aaviku küla and Aaviku with locations very close to each other.
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]

# We then also update the authority input on the basis of this.
authority_updated <- merge(unique(authority_input),subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]
# We do this 3 times in total to check for several alphabetically consequtive locations.
diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
# Here we still got 32 locations.
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]

diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
# And finally still 29 more locations - e.g Schleswig and Schlesswig, and Saltsjöbaden and Saltsjö-Boo. Places that are likely the same location but were not yet connected by previous methods.
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]

# We finally update the data once again.
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]
```


```{r main outputs,echo=F,warning=F,include=F}

# We can then make summary tables on the basis of this. We can connect the new harmonized names from the geographic method with the previous names harmonised by the rules.
works4 <- merge(works3,unique(authority_updated[,.(harm_name,place_for_edits)]),by.x="place_for_edits2",by.y="place_for_edits",all.x=T)
# And then we can also add coordinates to this file based on the harmonised name.
works4_geo <- merge(works4,unique(authority_updated[!is.na(lat),.(harm_name,lat,lon)]),by.x="harm_name",by.y="harm_name",all.x=T)

# To check the results we can have a look at the harmonized variants of for the city of Tartu.
tartu_variants <- works4[harm_name=="Tartu",.N,.(harm_name,place_unlist)][order(-N)]
# In total these cover more than 90k books.
tartu_variants[,.(sum(N))]
# We write this out as summary to be checked.
#fwrite(tartu_variants,"../reports/testsets/testset_tartu_variants.tsv",sep="\t")

# We can make small simple authority sets.
harm_names <- unique(works4[,.(harm_name,place_for_edits,place_unlist)])
harm_locations <- unique(works4_geo[!duplicated(place_unlist)][!is.na(lat),.(harm_name,lat,lon)])

# And store them as tables to be merged into the curated dataset.
places_harmonized_new <- harm_names[order(harm_name)][,.(place_original=place_unlist,place_harmonized=harm_name)][!duplicated(place_original)]
places_geotagged_new <- harm_locations[,.(place_harmonized=harm_name,lat,lon)]

fwrite(places_harmonized_new,"../config/places/places_harmonized.tsv",sep="\t")
fwrite(places_geotagged_new,"../config/places/places_coordinates.tsv",sep="\t")

# If the harmonization rules have been updated, the results can be compared with older versions of harmonization.
# 
# places_harmonized_old <- fread("../config//places/places_harmonized_old.tsv",sep="\t")
# places_geotagged_old <- fread("../config//places/places_coordinates_old.tsv",sep="\t")
# 
# places_geotagged_old[,uniqueN(place_harmonized)]
# places_geotagged_new[,uniqueN(place_harmonized)]
# 
# 
# check <- merge(harm_names[order(harm_name)][,.(place_original=place_unlist,place_harmonized=harm_name)],places_harmonized_old,by="place_original",all=T)
# check2 <- check[place_harmonized.x!=place_harmonized.y]
# 
# check3 <- merge(check2,places_geotagged_new,by.x="place_harmonized.x",by.y="place_harmonized",all.x=T)
# check4 <- merge(check3,places_geotagged_old,by.x="place_harmonized.y",by.y="place_harmonized",all.x=T)


# We can also create an overview table of the work ids with all their connected publication place name. 
works_harmonized <- works4[,.(id,publication_place_original=place_unlist,publication_place_harmonized=harm_name)]
#fwrite(works_harmonized,"../reports/works_publication_place_harmonized.tsv",sep="\t")


# After the processing 60 harmonized locations have not yet been resolved.
need_resolving5[,uniqueN(place_for_edits)]
#2449 unique harmonized locations have been given on the basis of 5249 place name variants
harm_names[,uniqueN(harm_name)]
harm_names[,uniqueN(place_unlist)]
harm_locations[,uniqueN(harm_name)]
works1[,uniqueN(place_unlist)]

#harm_names2 <- unique(works4[,.(harm_name,place_for_edits)])
#works4 <- merge(works3,harm_names2,by.x="place_for_edits2",by.y="place_for_edits",all.x=T)
#works4_geo <- merge(works4,harm_locations,by.x="harm_name",by.y="harm_name",all.x=T)

# Finally the results of the tagging can be used to find additional locations that do not yet have a location linked to them. They may be included by adding more rules to the rulebased approach or adding more locations to the geographic approach.
unresolved_names <- works4_geo[is.na(lat)][,.N,.(place_for_edits)]
#fwrite(unresolved_names,"../reports/testsets/unresolved names.tsv",sep="\t")
```


```{r testsets,echo=F,warning=F,include=F}

# Finally we can verify the results that we have.
library("rnaturalearth")
library(sf)
world <- ne_countries(scale = "medium", returnclass = "sf")

# For example we can plot all or main locations on the map.
top_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)]#[top_areas,on="publication_place_control"]
plot1 <- ggplot(data = world) +
    geom_sf(alpha=0.2) +
    geom_point(data = top_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
        shape = 20) +
    scale_size(trans="log10") +
    theme_bw()+
    labs(x="",y="")# +
    #coord_sf(xlim = c(-100, 78), ylim = c(24.5, 83), expand = FALSE)


# We can get an overview of the places with or without coordinates.
#works4_geo[!is.na(lat)][,.N,.(place_unlist)][,.N]
#works4_geo[!is.na(lat)][,uniqueN(id)]
#works4_geo[!is.na(lat)][,uniqueN(id)]/works4_geo[,uniqueN(id)]
#works4_geo[is.na(lat)][,uniqueN(id)]
#works4_geo[is.na(lat)][,.N,.(place_for_edits)]

# We can create a random sample of the locations and check whether the place name matches the location. We did this for 200 names in the article on the dataset.
sample_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)][,.(N=sum(N),publication_place_control=publication_place_control[1]),.(harm_name,lat,lon)][sample(.N,200)][order(-N)]

# We wrote this out as a separate file where true or false values could be checked
#fwrite(sample_places3,"data/sample_places_for_checking.tsv",sep="\t")
plot1 <- ggplot(data = world) +
    geom_sf(alpha=0.2) +
    geom_point(data = sample_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
        shape = 20) +
    scale_size(trans="log10") +
    theme_bw()+
    labs(x="",y="")

# And checked the locations on an interactive graph
library(htmlwidgets)
library(plotly)
#saveWidget(ggplotly(plot1), file=paste0( getwd(), "/sample_locations_on_map.html"))

# In some cases, it may be necessary to check the works themselves for which location may be the most accurate one and whether the publication_place_control in fact is correct
works4[place_unlist=="Palermo"]
works4[place_unlist=="Novi Sad"]
# This can be done also by work id.
works[id=="b13096497"]

# We stored the results of our manual verification in a separate tsv file and could check the results there.
checked_places <- fread("../reports/testsets/testset_places_checked.tsv",sep="\t")
# Since we looked at 200 samples, we can divide the number of true samples by 2 to get a percentage of correct samples. In our case this was 96%.
checked_places[,.N/2,true_place][true_place=="T"][,V1]


# There are still around 4000 works with no linked harmonised name.
works4[is.na(harm_name)][,uniqueN(id)]
works4[is.na(harm_name)&!is.na(place_for_edits)][,uniqueN(id)]

# For 284518 books, at least one of the associated placenames was exactly the same as the harmonized name
works4[harm_name==place_unlist][,uniqueN(id)]

# For 77479 books at least one of the names was altered as a result of harmonization.
works4[harm_name!=place_unlist][,uniqueN(id)]
# This forms a total of 24% of the books that had their palcename altered.
works4[harm_name!=place_unlist][,uniqueN(id)]/works4[,uniqueN(id)]

# As a result of the workflow it is possible to create a table of the works with all their associated harmonized placenames.
works_harmonized <- works4[,.(id,place=place_unlist,harm_name)]
#fwrite(works_harmonized,"data/works_harmonized_place.tsv",sep="\t")



```


```{r interactive plots,echo=F,warning=F,include=F}

library(networkD3)
# For visual illustrations we also created a network graph to be explored that shows the mappings.
mappings <- unique(works4[,.(harm_name,place_unlist)][!duplicated(place_unlist)])[!is.na(harm_name)]
data <- data_frame(mappings[,.(from=harm_name,to=place_unlist)])
p <- simpleNetwork(data, height="1000px", width="1000px",zoom=T)

# And saved it as html. Note that knitting the document does not create a selfcontained html but instead creates a folder with the tools used in the graph.
library(htmlwidgets)
saveWidget(p, file=paste0( getwd(), "/../reports/interactive_plots/places_harmonized_network.html"))




# We also created an interactive map of all the harmonized names and their locations in the dataset.
library("rnaturalearth")
library(sf)
world <- ne_countries(scale = "medium", returnclass = "sf")

top_places3 <- works4_geo[,.N,.(harm_name,publication_place_control,lat,lon)][order(-N)]#[top_areas,on="publication_place_control"]
plot1 <- ggplot(data = world) +
  geom_sf(alpha=0.2) +
  geom_point(data = top_places3, aes(x = lon, y = lat,size=N, text=paste0(harm_name, " (n = ", N, ")"), color = publication_place_control),  
             shape = 20) +
  scale_size(trans="log10") +
  theme_bw()+
  labs(x="",y="")# +
#coord_sf(xlim = c(-100, 78), ylim = c(24.5, 83), expand = FALSE)


# And also stored this as an interactive graph.
library(plotly)
saveWidget(ggplotly(plot1), file=paste0( getwd(), "/../reports/interactive_plots/places_on_map.html"))


# Finally we can look at some useful statistics.

# The number of place name variants
all_places[,uniqueN(place_unlist)]
# The number of variants after rule based harmonization
all_places[,uniqueN(place_for_edits)]

# The number of place name variants that were harmonized.
mappings[,uniqueN(place_unlist)]
# The number of unique names after harmonization
mappings[,uniqueN(harm_name)]
# The number of rules used for the rulebased approach
nrow(places_rules)

# The number of matches made from each geographic database.
compare_methods3[,uniqueN(place_for_edits)]
compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)] 
compare_methods3[!is.na(V5),uniqueN(place_for_edits)] 
compare_methods3[!is.na(publication_place_control),uniqueN(place_for_edits)] 

# The number of locations with matching or different geographic locations
compare_methods3[diff<0.2,uniqueN(place_for_edits)]
compare_methods3[diff>=0.2,uniqueN(place_for_edits)]
# And the number of works that had at least one geographic location associated with them.
scales::comma(works4_geo[!is.na(lat)][,uniqueN(id)])
```



```{r  compare with non-harmonized geotagging,echo=F,warning=F,include=F}

# Finally, we can also check the impact of the rulebased harmonization to the whole harmonization process.

# We can go through the geographic harmonization approach without applying the rules beforehand. This may still give good results as the apis we used often relied on a fuzzy search. Note that as place names are added, also apis ought to be queried again. In this study, the queries were made in setting up the workflow, a few recent additions to the set may not have been queried from the fuzzy match. Through updating this, both rulebased and simple approach can be improved further.

# We store the previous results as comparison sets.
works4_comp <- works4
works4_geo_comp <- works4_geo
authority_updated_comp <- authority_updated

# And make a new list of places based on unaltered place name variants.
places <- works3[,.N,.(place_for_edits=place_unlist)]

# The rest of the process is the same as based on the rulebased approach, so we will not comment it in detail here.
cities <- fread("../config/geoinfo/places_geonames_dump_cities500.txt")
cities2 <- cities[,.(variant_names=unlist(str_split(V4,","))),by=names(cities)]
biggest_pop_variant <- cities2[order(-V15),.SD[1],variant_names]
rm(cities,cities2)

places_w_geonames_global <- merge(places, biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)
ee_places <- fread("../config/geoinfo/places_geonames_dump_EE.txt")
ee_towns <- ee_places[V7=="P"]
ee_towns2 <- ee_towns[,.(variant_names=unlist(str_split(V4,","))),by=names(ee_towns)]
ee_biggest_pop_variant <- ee_towns2[variant_names=="",variant_names:=V2][order(-V15),.SD[1],variant_names]
rm(ee_places,ee_towns,ee_towns2)

places_w_geonames_ee <- merge(places[,.(place_for_edits,N)], ee_biggest_pop_variant, by.x="place_for_edits", by.y="variant_names",all.x=T)
check_missing <- places_w_geonames_ee[is.na(V1),.(place_for_edits,N)]

places_w_arcgis <- fread("../config/geoinfo/places_arcgis_api.tsv",sep="\t")

merge_method <- rbind(places_w_geonames_global[!is.na(V1)],places_w_geonames_ee[!is.na(V1)],fill=T)
merge_method[,duplicates:=.N,.(place_for_edits)]
check_duplicates <-merge_method[duplicates>1][order(place_for_edits)]
check_duplicates[,exact:=ifelse(length(unique(V1))==1,yes=T,no=F),by=place_for_edits]

for_sorting <- check_duplicates[exact!=T]
for_sorting[V9=="EE",count_ee:=.N,place_for_edits]
prefer_in_ee <- for_sorting[count_ee==1]
both_ee <- for_sorting[count_ee==2]
prefer_higher_pop_in_ee <- both_ee[order(-V15),.SD[1],place_for_edits]

keep <- rbind(merge_method[duplicates==1],check_duplicates[exact==T,.SD[1],place_for_edits],prefer_in_ee,prefer_higher_pop_in_ee,fill=T)

tagged1 <- fread("../config/geoinfo/places_geonames_api.tsv",sep="\t")[!is.na(lon)]
tagged2 <- fread("../config/geoinfo/places_google_api.tsv",sep="\t")[!is.na(lon)]

places2 <- works3[,.N,.(publication_place_control,place_for_edits=place_for_edits2,place_unlist)][order(place_for_edits)]

combine_data <- merge(places2[,.(place_for_edits=place_unlist,publication_place_control,N)],keep[,-c("N")],by="place_for_edits",all.x=T)
compare_methods <- merge(combine_data,tagged1[,.(location,geonames_lat=lat,geonames_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods2 <- merge(compare_methods,tagged2[,.(location,google_lat=lat,google_lon=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- merge(compare_methods2,places_w_arcgis[,.(location,arcgis_lon=lat,arcgis_lat=lon)], by.x="place_for_edits",by.y="location",all.x=T)
compare_methods3 <- compare_methods3[order(-N)][!is.na(place_for_edits)]

compare_methods3[!is.na(geonames_lat),harmonize_geonames:=place_for_edits[1],.(geonames_lat,geonames_lon)]
compare_methods3[!is.na(google_lat),harmonize_google:=place_for_edits[1],.(google_lat,google_lon)]
compare_methods3[!is.na(arcgis_lat),harmonize_arcgis:=place_for_edits[1],.(arcgis_lat,arcgis_lon)]


compare_methods3[,lat_diff:=max(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T)-min(V5,geonames_lat,google_lat,arcgis_lat,na.rm=T),by=place_for_edits][,lon_diff:=max(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T)-min(V6,geonames_lon,google_lon,arcgis_lon,na.rm=T),by=place_for_edits]
compare_methods3[,diff:=lat_diff+lon_diff]

check <- melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff"))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)]

melt_coords <- dcast(unique(melt( compare_methods3[diff>0.2][,.(place_for_edits,publication_place_control,N,diff,geodump_lat=V5,geodump_lon=V6,geonames_lat,geonames_lon,google_lat,google_lon,arcgis_lon,arcgis_lat,geonames_name=V2)],id.vars = c("place_for_edits","geonames_name","publication_place_control", "N","diff")))[, c("Source","lon_lat"):=tstrsplit(variable,"_(?=(lat|lon)$)", perl = TRUE)], place_for_edits + publication_place_control + N + diff + Source + geonames_name ~ lon_lat)
coord_options <- melt_coords[!is.na(lat)]
big_diffs <- coord_options[diff>0.2]
est_places <- big_diffs[lat<60&lat>57&lon>21&lon<29][,est_loc:=T]
with_est_loc <- merge(big_diffs,unique(est_places[,.(place_for_edits,est_loc)]),by="place_for_edits",all.x=T)

est_preferred_where_possible <- with_est_loc[est_loc==F|(lat<60&lat>56&lon>21&lon<29)]
est_preferred_where_possible[,newdiff:=(max(lat)-min(lat))+(max(lon)-min(lon)),by=place_for_edits]

loc_not_in_est <- with_est_loc[is.na(est_loc)]
compare_methods3[,any_lat:=V5][is.na(any_lat),any_lat:=geonames_lat][is.na(any_lat),any_lat:=google_lat][is.na(any_lat),any_lat:=arcgis_lat]
compare_methods3[,any_lon:=V6][is.na(any_lon),any_lon:=geonames_lon][is.na(any_lon),any_lon:=google_lon][is.na(any_lon),any_lon:=arcgis_lon]

matching_places <- compare_methods3[diff<0.2][,.(place_for_edits,geonames_name=V2,N,lat=any_lat,lon=any_lon)]
est_preferred_places <- est_preferred_where_possible[newdiff<0.2][!is.na(lat),.(lat=lat[1],lon=lon[1]),.(place_for_edits,geonames_name,N)]
need_resolving <- rbind(loc_not_in_est,est_preferred_where_possible[newdiff>0.2],fill=T)

countries <- fread("../config/geoinfo/marc_country_codes.tsv",sep="\t")[,code:=trimws(code)]
need_resolving2 <- merge(need_resolving[,publication_place_control:=trimws(publication_place_control)],countries,by.x="publication_place_control",by.y="code",all.x=T)

library(maps)
need_resolving3<-need_resolving2[,country_revgeo:=map.where(database="world", lon, lat)]
resolved_manually <- fread("../config/geoinfo/resolved_manually_geo.tsv",sep="\t")[,resolved:=T]
resolved_manually_for_merge <- resolved_manually[,.SD[1],.(place_for_edits,publication_place_control)]

need_resolving4 <- need_resolving3[!resolved_manually_for_merge,on=c("place_for_edits","publication_place_control")][!str_detect(place_for_edits,"S. ?l|Б.м.")]
add_some_estonian_locations <- need_resolving4[publication_place_control=="er"&str_detect(country_revgeo,"Estonia")][,.SD[1],.(place_for_edits,publication_place_control)]
need_resolving5 <- need_resolving4[!add_some_estonian_locations,on=c("place_for_edits","publication_place_control")]
  
add_some_estonian_locations <- need_resolving4[publication_place_control=="er"&country_revgeo=="Estonia"][,.SD[1],.(place_for_edits,publication_place_control)]
need_resolving5 <- need_resolving4[!add_some_estonian_locations,on=c("place_for_edits","publication_place_control")]

authority_input <- rbind(matching_places,est_preferred_places,resolved_manually_for_merge,add_some_estonian_locations,fill=T)
authority_input[,lat:=round(round(lat,2)*5,1)/5][,lon:=round(round(lon,2)*5,1)/5]
authority_input[!str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=T][str_detect(place_for_edits,"\\p{Cyrillic}"),latin:=F]
authority_input <- rbind(authority_input[latin==T],authority_input[latin==F])
authority_input[!is.na(geonames_name)&!(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=geonames_name]
authority_input[is.na(geonames_name)|(lat<60&lat>57&lon>21&lon<29),place_for_edits2:=place_for_edits]
authority_input[!is.na(lat),harm_name:=place_for_edits2[1],.(lat,lon)]

diffs_more <- unique(authority_input[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]

subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]

authority_updated <- merge(unique(authority_input),subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]
diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]

diffs_more <- unique(authority_updated[!is.na(lat),.(N=sum(N,na.rm=T)),.(harm_name,lat,lon)])[order(harm_name)][,diff_prev:=abs(lat-shift(lat))+abs(lon-shift(lon))][,prev_name:=shift(harm_name)][,prev_lat:=shift(lat)][,prev_lon:=shift(lon)][,prev_N:=shift(N)]
subs <- diffs_more[diff_prev<0.1][prev_N>N,preferred_name:=prev_name][!prev_N>N,preferred_name:=harm_name][,new_lat:=prev_lat][,new_lon:=prev_lon]
authority_updated <- merge(authority_updated[,-c("preferred_name","new_lat","new_lon")],subs[!is.na(harm_name),-c("N","lat","lon")],by="harm_name",all.x=T)[!is.na(new_lat),lat:=new_lat][!is.na(new_lon),lon:=new_lon][!is.na(preferred_name),harm_name:=preferred_name]


works4 <- merge(works3,unique(authority_updated[,.(harm_name,place_for_edits)]),by.x="place_unlist",by.y="place_for_edits",all.x=T)
works4_geo <- merge(works4,unique(authority_updated[!is.na(lat),.(place_for_edits,lat,lon)]),by.x="place_unlist",by.y="place_for_edits",all.x=T)

# Now that we have generated new equivalent datasets based on the raw name variants, we can compare them.
maincomparison <- merge(unique(works4_geo_comp[,.(place_unlist,harm_name_rules=harm_name,lat_harm=lat,lon_harm=lon)]),unique(works4_geo[,.(place_unlist,harm_name_raw=harm_name,lat_raw=lat,lon_raw=lon)]),by="place_unlist")[!duplicated(place_unlist)]

# We can lok at the differences in locations
maincomparison[,diff:=(lat_harm-lat_raw)+(lon_harm-lon_raw)]
maincomparison[lat_harm!=lat_raw|lon_harm!=lon_raw]

# And if this difference is significant then consider it a different match
differents <- maincomparison[diff>0.2|(is.na(lat_raw)&!is.na(lat_harm))]
# 243 place names have a match that is present in the rulebased approach that is not present in the raw approach
differents[is.na(lat_raw)&!is.na(lat_harm)]
# There are no locations with a coordinate with the raw approach but no coordinate with the rulebased approach.
add_to_harm <- differents[!is.na(lat_raw)&is.na(lat_harm)]
# In total the share of locations that had their coordinates added or improved from the whole is 7% here.
scales::percent(nrow(differents)/nrow(maincomparison),1)
# And this is based on 368 place name variants that includes both the ones that had a different location given to them as well as those without a location at first.
nrow(differents)

```


In all, there were `r all_places[,uniqueN(place_unlist)]` place name variants in the dataset. We harmonized  `r mappings[,uniqueN(place_unlist)]` of these variants to match `r mappings[,uniqueN(harm_name)]` harmonized placenames, while applying `r nrow(places_rules)` rules and exceptions based on regular expressions and variant matches to clean the dataset. After applying these methods, there were a total of `r all_places[,uniqueN(place_for_edits)]` unique placenames in the dataset. `r compare_methods3[!is.na(arcgis_lat),uniqueN(place_for_edits)]` of them could be geocoded with ArcGIS, `r compare_methods3[!is.na(google_lat),uniqueN(place_for_edits)]` of them with Google, `r compare_methods3[!is.na(geonames_lat),uniqueN(place_for_edits)]` of them with Geonames, and `r compare_methods3[!is.na(V5),uniqueN(place_for_edits)] ` in working with a local copy of Geonames with historical placenames. `r compare_methods3[diff<0.2,uniqueN(place_for_edits)]` had all the acquired coordinates within approximately 20 km from within each other and were considered non-conflicting matches. `r compare_methods3[diff>=0.2,uniqueN(place_for_edits)]` placenames showed bigger differences. In this case, if any of the placenames were in or near Estonia, they were preferred, providing a solution for `r est_places[,uniqueN(place_for_edits)]` places with no conflicts on coordinates in Estonia. Of these, `r need_resolving2[,uniqueN(place_for_edits)]` conflicting placenames were then resolved manually, in preferring the geographic match that was in or nearby the area marked in the MARC data format. `r need_resolving5[,uniqueN(place_for_edits)]` placenames could not be reasonably resolved to particular coordinates with this method. Through the process `r scales::comma(works4[harm_name!=place_unlist][,uniqueN(id)])` books were given a new name that was either cleaned up or grouped together with other name variants.

As a result, `r works4_geo[!is.na(lat)][,.N,.(place_unlist)][,.N]` unique placenames in the dataset have been given coordinates, providing a coordinate for `r scales::percent(works4_geo[!is.na(lat)][,uniqueN(id)]/works4_geo[,uniqueN(id)],3)` of the books (n = `r scales::comma(works4_geo[!is.na(lat)][,uniqueN(id)])`), `r need_resolving5[,uniqueN(place_for_edits)]` placenames there have not been resolved, and `r works3[order(-place_for_edits),.SD[1],id][place_for_edits=="",uniqueN(id)]` books in the dataset do not include information on the place of publication. Harmonizing place names enabled more accurate geocoding results for non-standard spellings (coordinates were added or corrected for `r nrow(differents)` place names, or `r scales::percent(nrow(differents)/nrow(maincomparison),1)` of the set). The harmonization and linking of the places of publication is likely to contain some errors, but a manual verification showed the results to be reasonably accurate (`r checked_places[,.N/2,true_place][true_place=="T"][,V1]`% of 200 randomly selected placenames showed accurate coordinates).  

